{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Assignment"
      ],
      "metadata": {
        "id": "aHtwph9YwkKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1 : What is simple linear regression ?"
      ],
      "metadata": {
        "id": "G4RxYxY9wyuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= Simple linear regression is a statistical method that models the relationship between a single dependent (outcome) variable and one independent (predictor) variable using a straight line (\\(y=\\beta _{0}+\\beta _{1}x+\\epsilon \\)). It estimates the best-fitting line to predict future outcomes, determining how much the dependent variable changes with each unit increase in the independent variable."
      ],
      "metadata": {
        "id": "p67OiPx0xnKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2 : What are the key assumption of simple linear regression ?"
      ],
      "metadata": {
        "id": "aKrujmhMx0vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= The key assumptions of simple linear regression are linear relationship between variables, independence of observations, homoscedasticity (constant variance of residuals), and normality of error distribution. Additionally, it assumes independent variables are measured without error and residuals have a mean of zero. Linearity: A straight-line relationship exists between the independent (\\(X\\)) and dependent (\\(Y\\)) variables.Independence: Observations are independent of each other (no autocorrelation).Homoscedasticity: The residuals (error terms) have constant variance across all levels of the independent variable.Normality: The residuals of the model are normally distributed.No Significant Outliers: Extreme values that can disproportionately influence the regression line should be minimized."
      ],
      "metadata": {
        "id": "awuouRv2yjO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3 : What does the coefficient m represent in the equation Y = mX + c ?"
      ],
      "metadata": {
        "id": "vGCHOFaAyzu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= In the linear equation \\(y=mx+c\\), the 'm' represents the slope (or gradient) of the line, indicating its steepness and direction, while 'c' is the y-intercept, the point where the line crosses the y-axis. So, 'm' tells you how much 'y' changes for every one unit increase in 'x'.                   What 'm' signifies:    Steepness: A larger absolute value of 'm' means a steeper line. Direction: If \\(m>0\\), the line goes up from left to right; if \\(m<0\\), it goes down; if \\(m=0\\), the line is horizontal.  Calculation: It's the ratio of the change in y-coordinates to the change in x-coordinates between any two points on the line (rise over run).              Example:    In \\(y=2x+3\\), the slope (\\(m\\)) is 2, meaning for every 1 unit you move right on the x-axis, the line goes up 2 units on the y-axis."
      ],
      "metadata": {
        "id": "x_W3L-h8zSc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*4 : What does the intercept c represent in the equation Y=mX+c ?"
      ],
      "metadata": {
        "id": "TTZcXFR6zdwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= In the linear equation \\(y=mx+c\\), the intercept \\(c\\) represents the y-intercept, which is the point where the line crosses or intersects the y-axis. It indicates the value of \\(y\\) when \\(x=0\\), representing the vertical shift or the starting value of the line on a Cartesian plane. Key aspects of the y-intercept (\\(c\\)) include: Coordinate Point: The line always crosses the y-axis at the coordinate point \\((0,c)\\).Vertical Position: It determines where the line intersects the vertical axis; if \\(c=0\\), the line passes through the origin \\((0,0)\\).Constant Term: In a linear equation, \\(c\\) is the constant term that shifts the line up (if positive) or down (if negative).Physical Significance: In real-world applications (e.g., \\(y=mx+c\\)), \\(c\\) often represents the initial value, fixed cost, or starting point before any change (dependent variable \\(y\\) when independent variable \\(x\\) is zero)."
      ],
      "metadata": {
        "id": "IZ5Dh0OG0AIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5 : How do we calculate the slope m in simple linear regression ?"
      ],
      "metadata": {
        "id": "_tZhU75N0RkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple linear regression, the slope (\\(m\\) or \\(\\beta _{1}\\)) represents the change in the dependent variable (\\(y\\)) for a one-unit increase in the independent variable (\\(x\\)). It is calculated using the Ordinary Least Squares (OLS) method, defined as the ratio of the covariance of \\(x\\) and \\(y\\) to the variance of \\(x\\). Formula:\\(m=\\frac{\\sum _{i=1}^{n}(x_{i}-\\={x})(y_{i}-\\={y})}{\\sum _{i=1}^{n}(x_{i}-\\={x})^{2}}\\) \\(\\sum \\): Summation symbol (sum over all data points)\\(x_{i},y_{i}\\): Individual data points\\(\\={x},\\={y}\\): Mean of \\(x\\) and \\(y\\) values, respectivelyNumerator: Sum of products of deviations (Covariance proportional)Denominator: Sum of squared deviations for \\(x\\) (Variance proportional) Key Notes: A positive \\(m\\) indicates a positive relationship (upward trend), while a negative \\(m\\) indicates a downward trend.It can also be calculated as \\(m=r\\frac{s_{y}}{s_{x}}\\) (Correlation \\(\\times \\) Ratio of Standard Deviations).The slope represents the \"rise over run\" for the best-fit line."
      ],
      "metadata": {
        "id": "7h1y77Ek0zyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6 : What is the purpose of the least squares method in simple linear regression ?"
      ],
      "metadata": {
        "id": "N9YuPES-03jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= The purpose of the least squares method in simple linear regression is to find the \"line of best fit\" by minimizing the sum of the squared vertical distances (residuals) between observed data points and the fitted line. This minimizes overall prediction error, enabling accurate modeling of the relationship between variables. Key points regarding the method's purpose: Minimizing Error: It reduces the sum of the squares of the errors (vertical differences), known as residuals.Best Fit Line: It determines the unique, optimal line (\\(y=a+bx\\)) that represents the trend in the data.Parameter Estimation: It estimates the slope and intercept coefficients by minimizing the deviation of data points from the model.Predictive Modeling: The derived line is used to predict the dependent variable for a given independent variable."
      ],
      "metadata": {
        "id": "ahDjm1-B1bfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*7 : How is the corfficient of determination (R2) interpreted in simple linear regression ?"
      ],
      "metadata": {
        "id": "7HJlhodS1evB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= The coefficient of determination (\\(R^{2}\\)) represents the proportion of the total variation in the dependent variable \\(y\\) that is explained by the independent variable \\(x\\) in a linear regression model. Statistical Meaning In simple linear regression, \\(R^{2}\\) serves as a measure of goodness of fit, indicating how well the regression line approximates the observed data points. It is defined by the formula:\\(R^{2}=\\frac{SSR}{SST}\\)where \\(SSR\\) is the sum of squares regression (explained variation) and \\(SST\\) is the total sum of squares (total variation). Key Interpretations Scale: The value ranges from 0 to 1. A value of \\(0\\) means the model explains none of the variability, while a value of \\(1\\) means the model explains all of the variability.Percentage: It is often expressed as a percentage. For example, an \\(R^{2}=0.85\\) means that 85% of the variance in the dependent variable is predictable from the independent variable.Correlation Link: In simple linear regression, \\(R^{2}\\) is exactly the square of the Pearson correlation coefficient (\\(r\\)).Limitations: While a high \\(R^{2}\\) indicates a strong fit, it does not imply causation or indicate whether the chosen model is the most appropriate for the data."
      ],
      "metadata": {
        "id": "EPBfpTMJ3FfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*8 : What is multiple linear regression ?"
      ],
      "metadata": {
        "id": "WASd4TxP3dup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=  (MLR) is a statistical technique that models the linear relationship between one continuous dependent variable and two or more independent (predictor) variables. It extends simple linear regression by using multiple predictors to estimate outcomes, assuming a straight-line relationship in multidimensional space. Key Aspects of Multiple Linear Regression: Formula: The model is represented by the equation \\(y=\\beta _{0}+\\beta _{1}x_{1}+\\beta _{2}x_{2}+...+\\beta _{k}x_{k}+\\epsilon \\), where \\(y\\) is the dependent variable, \\(x_{i}\\) are independent variables, \\(\\beta _{i}\\) are coefficients, and \\(\\epsilon \\) is the error term.Purpose: It is used to predict outcomes and determine the significance of multiple explanatory variables on a dependent variable.Coefficient Interpretation: Each coefficient (\\(\\beta _{i}\\)) represents the average change in the dependent variable for a one-unit increase in that specific predictor, assuming all other variables are held constant.Assumptions: For the model to be valid, there must be a linear relationship, independent variables should not be highly correlated (no multicollinearity), and residuals should have constant variance.Applications: Commonly used in fields like economics, finance, and health sciences to predict trends or analyze the impact of multiple factors (e.g., predicting house prices based on size, location, and age). MLR is a fundamental tool in supervised machine learning used for modeling complex, multi-factor scenarios."
      ],
      "metadata": {
        "id": "iS0BMc3031OO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*9 : What is the main difference between simple and multiple linear regression ?"
      ],
      "metadata": {
        "id": "7KgS_43M31Hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= The main difference is the number of independent variables used to predict the dependent variable. Simple linear regression uses one independent variable to model the relationship, while multiple linear regression uses two or more independent variables to provide a more accurate, complex analysis. Simple Linear Regression: \\(Y=\\beta _{0}+\\beta _{1}X_{1}+\\epsilon \\)Multiple Linear Regression: \\(Y=\\beta _{0}+\\beta _{1}X_{1}+\\beta _{2}X_{2}+...+\\beta _{n}X_{n}+\\epsilon \\) Key Takeaways: Complexity: Multiple regression accounts for multiple factors, whereas simple only considers one.Predictive Power: Multiple linear regression generally offers better, more nuanced insights by evaluating combined effects.Usage: Simple is used for basic, two-variable relationships; multiple is used for complex, real-world data."
      ],
      "metadata": {
        "id": "Mk35sXdJ4RPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*10 : What are the key assumptions of multiple linear regression ?"
      ],
      "metadata": {
        "id": "oYLmrhLl4jqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= The key assumptions for multiple linear regression (MLR) ensure valid results and are Linearity, Homoscedasticity, Independence of Errors, Normality of Errors, and No Multicollinearity, stating the relationship is linear, error variance is constant, errors are uncorrelated, errors are normally distributed, and independent variables aren't perfectly related to each other, respectively, all critical for reliable predictions and inference.\n",
        "Key Assumptions of MLR\n",
        "Linearity: A straight-line relationship exists between the dependent variable and the independent variables.\n",
        "Check: Residual plots (residuals vs. predicted values) should show a random scatter, not curves or patterns.\n",
        "Homoscedasticity (Constant Variance): The variance of the errors (residuals) is the same across all levels of the independent variables.\n",
        "Check: Residual plots should show a consistent spread (no funnel shape).\n",
        "Independence of Errors: The errors (residuals) are not correlated with each other (no autocorrelation).\n",
        "Check: The Durbin-Watson test for time-series data.\n",
        "Normality of Errors: The residuals are normally distributed for any given set of independent variables.\n",
        "Check: Histograms or Q-Q plots of residuals; tests like Kolmogorov-Smirnov.\n",
        "No Multicollinearity: Independent variables are not highly correlated with each other.\n",
        "Check: Variance Inflation Factor (VIF) (VIF > 10 is problematic) or correlation matrices.\n",
        "Additional (Implicit) Assumptions\n",
        "Variables are not random: The independent variables are fixed, not random, in the context of the model.\n",
        "Zero Mean Error: The expected value (mean) of the errors, given the predictors, is zero."
      ],
      "metadata": {
        "id": "-Y_dp6kX4_iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*11 : What is heteroscedasticity , and how does it affect the result of a multiple linear regression model ?"
      ],
      "metadata": {
        "id": "PGZwUyra5EmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= Heteroscedasticity occurs in multiple linear regression when the variance of the error terms (residuals) is not constant across all levels of the independent variables. Instead of a constant spread, the residuals show a \"fan\" or \"cone\" shape pattern. It violates the Ordinary Least Squares (OLS) assumption of homoscedasticity.\n",
        "Effects on Multiple Linear Regression Results:\n",
        "Inefficient Estimates: While OLS estimators remain unbiased, they are no longer efficient (i.e., not Minimum Variance Unbiased Estimators).\n",
        "Biased Standard Errors: The standard errors of the coefficients are typically biased (often underestimated), which leads to incorrect, unreliable t-tests and p-values.\n",
        "Invalid Inference: Confidence intervals may be too narrow or too wide, leading to incorrect inferences about the significance of predictor variables.\n",
        "Inefficient Predictions: Although predictions are still unbiased, the confidence intervals for predicted values will be incorrect.\n",
        "How to Address It:\n",
        "Transform the Dependent Variable: Using log transformation can stabilize variance.\n",
        "Use Robust Standard Errors: Using \"sandwich estimators\" (e.g., White's estimator) provides reliable, consistent standard errors, even with heteroscedasticity.\n",
        "Weighted Least Squares (WLS): Use this method if the nature of the heteroscedasticity is known."
      ],
      "metadata": {
        "id": "udi6ilPz51rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 12 : How can you improve a multiple linear regression model with high multicollinearity ?"
      ],
      "metadata": {
        "id": "MLxkEq3R55gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "= To improve a multiple linear regression model with high multicollinearity, one can use the following methods:\n",
        "Feature Selection: Remove one of the highly correlated independent variables from the model. This is the simplest approach and often effective when multiple variables convey redundant information [1].\n",
        "Dimensionality Reduction: Combine the correlated features into a single composite variable or use techniques such as Principal Component Analysis (PCA) to transform the original variables into a new set of uncorrelated components (principal components) [1, 2].\n",
        "Regularization Techniques: Employ regularization methods like Ridge Regression or the Lasso. These techniques add a penalty to the regression equation, which helps shrink the coefficient estimates towards zero and manage the impact of multicollinearity on the model's stability [1, 2].\n",
        "Partial Least Squares (PLS) Regression: Use a different type of regression method specifically designed to handle multicollinearity and high-dimensional data by finding new predictor variables (components) that explain both the independent and dependent variables [1].\n",
        "Increase Data Size: If feasible, collecting more data can sometimes dilute the effect of multicollinearity, as a larger sample size may provide more reliable estimates of the regression coefficients [1]."
      ],
      "metadata": {
        "id": "z6pqdZHz6kjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rJLi2kLA6oHU"
      }
    }
  ]
}